{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac49eb38-59c0-4afd-b7a0-6bcb96a4ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balaganapathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\balaganapathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110dca09-fb0c-4f03-a86d-44cab170d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout = 0.5, grad = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet = models.resnet50(weights='DEFAULT')\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
    "        \n",
    "        if not grad:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.resnet(x)\n",
    "        return self.dropout(self.relu(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ade5c7c-8993-4701-9b1e-f2f789773064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, device, encoder, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.encoder = encoder.to(device)\n",
    "    \n",
    "    def forward(self, image, caption):\n",
    "        features = self.encoder(image)\n",
    "        \n",
    "        embeddings = self.dropout(self.embed(caption))\n",
    "       \n",
    "        embeddings = torch.cat((features.unsqueeze(1),embeddings), dim=1)\n",
    "        \n",
    "        outputs, state = self.lstm(embeddings)\n",
    "        outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c807b6-6a4d-4f28-a5da-de3719994bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"captions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a48724-ca43-46e0-ab1f-74d6e2ef4e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eacca177-a818-47fb-9a36-6447965c9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, lowercase=False, remove_punc=False, remove_num=False, sos_token='<sos>', eos_token='<eos>'):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if remove_punc:\n",
    "        text = ''.join([ch for ch in text if ch not in punctuation])\n",
    "    if remove_num:\n",
    "        text = ''.join([ch for ch in text if ch not in '1234567890'])\n",
    "    text = [sos_token] + word_tokenize(text) + [eos_token]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e4d084-c498-4a00-87d0-c264df76a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190d0ef7-439b-43b5-aac7-f14e6d37e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balaganapathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e718612-1e6f-4d29-88c8-b417994603e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'a', 'cat', 'is', 'sitting', 'on', 'the', 'table', '<eos>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"A cat is sitting on the table.\", lowercase=True, remove_punc=True, remove_num=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2098339b-0101-4e6f-99e8-d9aaaf7aacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3050db9-36dc-486b-a38a-e21239844177",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cap = data['caption'].apply(lambda x: clean_text(x, lowercase=True, remove_punc=True, remove_num=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c4d8b4-f1b5-4f13-8609-77db72322241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [<sos>, a, child, in, a, pink, dress, is, clim...\n",
       "1    [<sos>, a, girl, going, into, a, wooden, build...\n",
       "2    [<sos>, a, little, girl, climbing, into, a, wo...\n",
       "3    [<sos>, a, little, girl, climbing, the, stairs...\n",
       "4    [<sos>, a, little, girl, in, a, pink, dress, g...\n",
       "Name: caption, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_cap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c781a3-8d67-4d24-b0f6-351667f9867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_caption'] = clean_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56af0f00-60db-4b8d-9c4d-0bd901c03097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>clean_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>[&lt;sos&gt;, a, child, in, a, pink, dress, is, clim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;sos&gt;, a, girl, going, into, a, wooden, build...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>[&lt;sos&gt;, a, little, girl, climbing, into, a, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>[&lt;sos&gt;, a, little, girl, climbing, the, stairs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>[&lt;sos&gt;, a, little, girl, in, a, pink, dress, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \\\n",
       "0  A child in a pink dress is climbing up a set o...   \n",
       "1              A girl going into a wooden building .   \n",
       "2   A little girl climbing into a wooden playhouse .   \n",
       "3  A little girl climbing the stairs to her playh...   \n",
       "4  A little girl in a pink dress going into a woo...   \n",
       "\n",
       "                                       clean_caption  \n",
       "0  [<sos>, a, child, in, a, pink, dress, is, clim...  \n",
       "1  [<sos>, a, girl, going, into, a, wooden, build...  \n",
       "2  [<sos>, a, little, girl, climbing, into, a, wo...  \n",
       "3  [<sos>, a, little, girl, climbing, the, stairs...  \n",
       "4  [<sos>, a, little, girl, in, a, pink, dress, g...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "319999de-05f8-4f51-b772-fb2d1861ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(clean_cap, specials=[unk_token, pad_token, sos_token, eos_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9684ca0-d29f-4059-b972-4e402f01fefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'a', 'in', 'the', 'on', 'is', 'and']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7329357a-33a3-4931-a24c-8d2971067f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_idx = vocab[pad_token]\n",
    "unk_token_idx = vocab[unk_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6df4afd1-99c4-446c-a10e-16e7ed6f4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4f8a1c-fa34-492c-addd-254695938c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to number\n",
    "def text_to_number(text, vocab):\n",
    "    return [vocab[token] for token in text]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ff5a38-9629-4a96-84cc-9856934485c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_int = clean_cap.apply(lambda x: text_to_number(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02334bce-7747-4adb-b415-9d68781de76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [2, 4, 43, 5, 4, 91, 171, 8, 120, 54, 4, 400, ...\n",
       "1                      [2, 4, 20, 316, 65, 4, 196, 118, 3]\n",
       "2                 [2, 4, 41, 20, 120, 65, 4, 196, 2569, 3]\n",
       "3             [2, 4, 41, 20, 120, 6, 394, 21, 61, 2569, 3]\n",
       "4        [2, 4, 41, 20, 5, 4, 91, 171, 316, 65, 4, 196,...\n",
       "                               ...                        \n",
       "40450         [2, 4, 12, 5, 4, 91, 38, 253, 4, 85, 124, 3]\n",
       "40451             [2, 4, 12, 8, 85, 120, 197, 5, 6, 66, 3]\n",
       "40452    [2, 4, 44, 5, 4, 26, 38, 120, 54, 4, 85, 124, ...\n",
       "40453                     [2, 4, 85, 359, 5, 4, 26, 38, 3]\n",
       "40454         [2, 4, 85, 359, 1915, 7, 4, 85, 120, 110, 3]\n",
       "Name: caption, Length: 40455, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d297c3-0333-433f-bbf0-760eeb344081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embed_caption'] = to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448c3af2-30b0-403f-8f7c-c6eb7486256e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>clean_caption</th>\n",
       "      <th>embed_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>[&lt;sos&gt;, a, child, in, a, pink, dress, is, clim...</td>\n",
       "      <td>[2, 4, 43, 5, 4, 91, 171, 8, 120, 54, 4, 400, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;sos&gt;, a, girl, going, into, a, wooden, build...</td>\n",
       "      <td>[2, 4, 20, 316, 65, 4, 196, 118, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>[&lt;sos&gt;, a, little, girl, climbing, into, a, wo...</td>\n",
       "      <td>[2, 4, 41, 20, 120, 65, 4, 196, 2569, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>[&lt;sos&gt;, a, little, girl, climbing, the, stairs...</td>\n",
       "      <td>[2, 4, 41, 20, 120, 6, 394, 21, 61, 2569, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>[&lt;sos&gt;, a, little, girl, in, a, pink, dress, g...</td>\n",
       "      <td>[2, 4, 41, 20, 5, 4, 91, 171, 316, 65, 4, 196,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \\\n",
       "0  A child in a pink dress is climbing up a set o...   \n",
       "1              A girl going into a wooden building .   \n",
       "2   A little girl climbing into a wooden playhouse .   \n",
       "3  A little girl climbing the stairs to her playh...   \n",
       "4  A little girl in a pink dress going into a woo...   \n",
       "\n",
       "                                       clean_caption  \\\n",
       "0  [<sos>, a, child, in, a, pink, dress, is, clim...   \n",
       "1  [<sos>, a, girl, going, into, a, wooden, build...   \n",
       "2  [<sos>, a, little, girl, climbing, into, a, wo...   \n",
       "3  [<sos>, a, little, girl, climbing, the, stairs...   \n",
       "4  [<sos>, a, little, girl, in, a, pink, dress, g...   \n",
       "\n",
       "                                       embed_caption  \n",
       "0  [2, 4, 43, 5, 4, 91, 171, 8, 120, 54, 4, 400, ...  \n",
       "1                [2, 4, 20, 316, 65, 4, 196, 118, 3]  \n",
       "2           [2, 4, 41, 20, 120, 65, 4, 196, 2569, 3]  \n",
       "3       [2, 4, 41, 20, 120, 6, 394, 21, 61, 2569, 3]  \n",
       "4  [2, 4, 41, 20, 5, 4, 91, 171, 316, 65, 4, 196,...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "301b8575-cdde-41a4-893e-f17918e964fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'a',\n",
       " 'child',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pink',\n",
       " 'dress',\n",
       " 'is',\n",
       " 'climbing',\n",
       " 'up',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'stairs',\n",
       " 'in',\n",
       " 'an',\n",
       " 'entry',\n",
       " 'way',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens(data['embed_caption'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "260c953d-8883-4ada-8001-1d1b13ca4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "974d2726-9d27-45c9-bf41-71b4d81fc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_index):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for img, cap in batch:\n",
    "        images.append(img)\n",
    "        captions.append(cap)\n",
    "    images = torch.stack(images)\n",
    "    captions = torch.nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=pad_index)\n",
    "    return images, captions\n",
    "\n",
    "def get_collate_fn(pad_index):\n",
    "    return lambda batch: collate_fn(batch, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7ae0126-c484-42a3-a06e-c71ee70f5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, data, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.captions = data['embed_caption']\n",
    "        self.images = data['image']\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.root_dir, self.images[idx]))\n",
    "        caption = torch.tensor(self.captions[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6c8341c-95ba-47ce-9c5b-f8e350ff3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # data type convert to tensor\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bad15fd-6720-440b-a5df-b7559bd7e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\"images\", train, transform=transform)\n",
    "test_dataset = CustomDataset(\"images\", test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef05fcbb-6c52-4921-9434-652ecdb0ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d275f35-bb8f-46e5-99e6-b0461869fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                               collate_fn=get_collate_fn(pad_token_idx))\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                              collate_fn=get_collate_fn(pad_token_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7021282-2027-4c4f-bcc6-b83673731474",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "hidden_dim = 200\n",
    "vocab_size = len(vocab)\n",
    "num_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd3262ae-cc71-4e43-b73d-6a9b89898f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_dim, dropout)\n",
    "model = Decoder(embed_dim, hidden_dim, vocab_size, num_layers, device, encoder, dropout )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3654c96d-c961-4a8c-b4a4-868c79470056",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_token_idx)\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "best_valid_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f31ac99-aea9-414b-b7e3-349a1ca6e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        images, captions = batch\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "     \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        captions_in = captions[:,:-1]\n",
    "        outputs = model(images, captions_in)\n",
    "        outputs = outputs.view(-1, outputs.shape[2]).to(device)\n",
    "        \n",
    "        captions = captions.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, captions)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b67ed316-93ef-4698-9ea0-b884104b178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            images, captions = batch\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            captions_in = captions[:,:-1]\n",
    "            outputs = model(images, captions_in)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.shape[2]).to(device)\n",
    "            captions = captions.view(-1)\n",
    "        \n",
    "            loss = criterion(outputs, captions)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac77328-bdd5-4f64-8901-9c25220857f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        device)\n",
    "    \n",
    "    \n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        test_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"best-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f}\")\n",
    "torch.save(model.state_dict(), \"last_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2556fd0-ac62-4a8e-a033-c94288265b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image = Image.open(\"Images/1015118661_980735411b.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93350ba-b817-4ce6-ac5b-37fe046da2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19c707-b3a5-4f14-b61a-6a5d3a6e890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_caption(model, image, vocab, max_length=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(image).unsqueeze(1)\n",
    "        input = features\n",
    "        hidden = torch.zeros(model.num_layers, 1, model.lstm.hidden_size).to(model.device)\n",
    "        cell = torch.zeros(model.num_layers, 1, model.lstm.hidden_size).to(model.device)\n",
    "\n",
    "        caption = []\n",
    "        for _ in range(max_length):\n",
    "            output, (hidden, cell) = model.lstm(input, (hidden, cell))\n",
    "            output = model.linear(output.squeeze(1))\n",
    "            predicted = output.argmax(1)\n",
    "            caption.append(predicted.item())\n",
    "            input = model.dropout(model.embed(predicted)).unsqueeze(1)\n",
    "            if predicted.item() == vocab['<eos>']:\n",
    "                break\n",
    "                \n",
    "    return vocab.lookup_tokens(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340b723-1890-48e2-922a-efb12600204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transform(image)\n",
    "pred = predict_caption(model, image.unsqueeze(0).to(device), vocab, 20)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58339f8e-a171-4dc2-b006-6d1a2a809ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead7baf-8d3f-4a7e-b8d6-59731846cff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
